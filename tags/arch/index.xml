<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arch on 沉风网事</title>
    <link>https://myself659.github.io/tags/arch/</link>
    <description>Recent content in Arch on 沉风网事</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 08 Apr 2017 11:58:06 +0200</lastBuildDate>
    
	<atom:link href="https://myself659.github.io/tags/arch/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>对于系统设计的一些想法</title>
      <link>https://myself659.github.io/2017/04/08/%E5%AF%B9%E4%BA%8E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%84%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/</link>
      <pubDate>Sat, 08 Apr 2017 11:58:06 +0200</pubDate>
      
      <guid>https://myself659.github.io/2017/04/08/%E5%AF%B9%E4%BA%8E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%84%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/</guid>
      <description>前言 学习了google，facebook等国际一流大厂的开源方案，也研究了国内BAT的一些设计案例，在平时工作自然也少不了一些系统设计的工作，想写一些自己的想法，同时也帮助自己梳理一下思路，实现自己的系统设计的套路。
其实就是这一句话：立足需求与业务，利用工程与技术，得到最合适的tradeofff，追求更简单的设计与方案，以此不断推进系统的演化。
下面对这句话展开说明（当然，下面是一堆费话，只是为了解决自己的不吐不快而已罢了）。
立足需求与业务 套用一句话：一切脱离需求的设计都是耍流氓。这里不进行具体案例分析，主要从以下角度来细化需求，提供思考的方向。
用户角度  performance（性能） availability（可用性） usability（易用性） security（安全性）  研发角度  maintainablity （可维护性性） protability（可移植性） reusability（可重用性） scalable(可扩展性) testability（可测试性）  商业与市场角度  time to market（及时发布推向市场） cost and benifits（成本和收益） projected life time （产品生命周期） targeted market（目标市场） integration with legacy system (系统集成) roll back schedule （回退时间表）  KISS(Keep It Simple, Stupid) 有太多的例子，说明追求简单与遵循简单的设计原则的重要性，最典型就是unix的设计哲学成就伟大的linux的操作系统。
什么是简单的系统设计呢？ 这是需要不断思考的问题，举个例子说明吧；在GFS实现中，针对client向chunk server写文件失败的问题，GFS的作法是直接返回失败，由client决定是否重写，这种作法就是聪明的简单之举。
简单并不是随手可得的。关于这个可以参考rob pike，golang发明人之一的这篇演进： Simplicity is Complicated
下面借此说明以下几个问题？
 什么是简单？  简单很难定义，还是举例说明吧 追求简单并不是单纯追求技术实现上的简单。简单追求是使用的简单，因为使用是高频，实现可能只有几次，例如上面演进谈到的GC,实现并不简单，想出这个GC算法就相当困难，实现那就更难了，但是有了GC，我们用golang编程的时候就不需要像C/C++那样关心内存的申请释放，再也不用担心踩内存的问题了，专心于设计与业务，给程序员带来了简单。（以我自己为例，学会了golang，我写代码都写得多，之前只会C/C++时候，业余时间主要是阅读代码，写代码都是工作驱动）
另外还有一点，简单是先实现，再改进，例如golang的GC算法一开始并不好，GC导致应用 延迟大，到了1.5才有改进
如何实现简单？  演进中const同c语言定义一个常量不一样，不需要关心类型，在生活中一般人说数字2017除了程序员谁关心它是整型数还是浮点数啊</description>
    </item>
    
    <item>
      <title>IM后端系统设计总结(2)</title>
      <link>https://myself659.github.io/2016/08/05/im%E5%90%8E%E7%AB%AF%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%80%BB%E7%BB%932/</link>
      <pubDate>Fri, 05 Aug 2016 11:58:06 +0200</pubDate>
      
      <guid>https://myself659.github.io/2016/08/05/im%E5%90%8E%E7%AB%AF%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%80%BB%E7%BB%932/</guid>
      <description>这篇具体写一下用户在线状态系统的具体设计。
后端架构 这个后端系统设计如下图：
很大众，国内基本都这么干，不多说
用户状态系统设计 初期设计 单IDC部署，设计如下:
相关说明  AG：接入网关，负责用户的连接 ConnRouter：连接路由服务器，主要提供以下功能：   所有用户状态的维护 用户状态查询 用户状态推送 用户状态同步 异步消息路由与转发  状态通知流: 用户登录成功或者下线状态通知 状态同步流：ConnRouter服务器之间用户状态的同步，不需要推送给订阅者 用户消息流: 异步发送给用户的消息在服务器内部的传输  设计要点  参考Kafka的模型，将用户状态改变作为事件，将事件描述为消息，将消息队列化成消息队列 AG对应Kafka中的Producer角色，主要原因是用户状态是用户连接的影子，AG能真实快速感知用户状态 ConnRouter对应Kafka中的Broker UserStat（用户统计与分析）与StateNotify（用户状态通知）对应Kafka中的Consumer 减少耦合，以异步发送消息到用户为例，整个流程三步走：   第一步：生成发送消息，发送到ConnRouter 第二步：根据目的用户ID，查找到出口AG，将消息转发到出口AG 第三步：出口AG查找用户连接，通过连接发送到目的用户  ConnRouter设计 用户状态数据存储设计 用户状态数据存储设计，如下图所示：
 内存消耗 主要内存消耗来自用户状态数组（与ConnRouter连接都是长连接，可以忽略不计） 每种客户端类型下每个用户占用一个1Byte，那么1G内存可以存1073741824个用户的状态，超过了10亿，支持亿级用户内存不是瓶颈 查找用户所在AG与状态O(1)  高可用性  服务器级Master-Master模式 根据用户ID选择ConnRouter Master实现用户级Master-Standby模式  高性能  epoll事件驱动 无锁数据访问 流程无阻塞 O(1)查找 多核并发 支持批量处理  支持Failover，方便升级  当ConnRouter宕机或者主动升级重启时，各AG重新建立连接时，将自身的用户状态同步到ConnRouter，完成用户状态数据恢复  无状态与单点自治  ConnRouter用户状态数据是各个AG的同步，真正用户状态保存在AG网关 单点可以独立工作  数据冲突处理 主要是一个数据优先级的原则（根据数据源，从高到低）：</description>
    </item>
    
    <item>
      <title>IM后端系统设计总结(1)</title>
      <link>https://myself659.github.io/2016/07/29/im%E5%90%8E%E7%AB%AF%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%80%BB%E7%BB%931/</link>
      <pubDate>Fri, 29 Jul 2016 11:58:06 +0200</pubDate>
      
      <guid>https://myself659.github.io/2016/07/29/im%E5%90%8E%E7%AB%AF%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%80%BB%E7%BB%931/</guid>
      <description>定义IM 为用户提供即时消息服务，这里面有三个关键词：用户，消息，服务；下面根据三个关键词来展开总结，先从消息开始。
消息 消息分类 对消息分类，很简单但是重要，方便后面业务的拆分。
 注册 登录 用户信息 聊天消息 群组 好友 文件 版本 内部服务器之间消息 客户端诊断 其他业务消息  信息语义描述 在互联网和移动互联网时代常见有以下几种方式：
 XML 文本 MQTT 自定义二进制（这里一般默认为google protobuf）  这四种类型比较如下表所示：
   比较项 XML 文本 MQTT 自定义二进制     可读性 好 好 差 差   通用性 标准协议，易通用 支持通用http协议，也可自定义 通用标准协议 私有协议，无法通用   扩展性 易扩展，支持第三方 易扩展 可扩展 仅协议可扩展，不支持第三方   流量消耗 极大 大 小 小   处理效率 低 一般 高 高   网络适应性 差 一般 较好 较好   业内应用 新浪微博/GTalk MSN facebook messenger QQ/weixin    再补充说一点，采用二进制协议，在网络带宽及消息存储方面可以节约成本，特别用户量达到千万级以上</description>
    </item>
    
    <item>
      <title>10 Lessons from 10 Years of Amazon Web Services（译文）</title>
      <link>https://myself659.github.io/2016/06/02/10-lessons-from-10-years-of-amazon-web-services%E8%AF%91%E6%96%87/</link>
      <pubDate>Thu, 02 Jun 2016 11:58:06 +0200</pubDate>
      
      <guid>https://myself659.github.io/2016/06/02/10-lessons-from-10-years-of-amazon-web-services%E8%AF%91%E6%96%87/</guid>
      <description>前言  亚马逊在2006年3月14日发布AWS，到现在差不多10年了。回首过去的10年里，我们在构建 安全，高可用性，可扩展性，低成本的服务方面积累了几百条经验与教训。 由于AWS是建设并在全球运营这些服务的先驱，这些教训对我们的业务至关重要。正如我们以前多次说，“没有压缩经验的算法”，每月有超过百万的活跃客户，这些客户服务几个亿的用户，在这个过程我们不乏机会积累经验并持续优化从而为客户提供更好的服务。
  我选择了下面这些经验教训，与大家分享，希望它们对你们有用。
 1. 构建不断进化的系统  几乎从第一天开始,我们知道自己开发的软件在一年后将不会继续运行。我们需要重新审视和改进架构，以确保我们可以解决订单规模增长一到二个数量级所带来的问题。但是我们不能采取停电检修这种旧方法升级系统，这是因为遍布世界各地的海量业务需要我们的平台提供7*24小时高可用性服务。我们需要建立这样架构：能够在不停止服务的情况下引入新的组件。Marvin Theimer,亚马逊杰出工程师，曾开玩笑地说，亚马逊S3的演变可以被描述为从当初的单引擎飞机，随着时间的推移飞机不断升级，先是升级到737，然后是一组747，现在成了3805的空中舰队。在此期间，我们在飞行过程中完成加油，而客户甚至没有觉察到这一点。
 感悟： 唯有变化才是确定，系统应该保证灵活性与扩展性。云计算系统是需要不断进化，开发人员也需要不断进化
2. 总有想不到的  异常总是会发生的，随着时间的推移，一切都会出现异常：从路由器到硬盘，从操作系统到内存单元，从瞬态错误到永久性故障。无论你使用的是最高品质的硬件或最低成本的硬件，这些总是会发生的。在大规模系统中更是如此，例如，在S3中处理和存储的万亿交易，任何异常，即使是最小的可能性也将成为现实。部分这样的异常可以事先预见的，但更多的异常却在设计和开发过程中未能被发现。
  我们需要开发一个视异常为常态的系统，即使我们不知道会有什么样的异常。即使“房子着火了”，系统也需要保持运行。重要的是在整个系统在不宕机的情况下能够处理这些异常。我们掌握隔离异常与控制异常的影响范围的方法从而使整个系统能够正常运行。
 感悟：异常是常态，如何处理异常是系统设计阶段必须考虑到的问题。
3. 提供基元而不是框架  我们很快意识到，客户的需求是不断变化的。当客户摆脱传统的IT硬件和数据中心的限制，他们开始使用感兴趣却没有应用过的模式搭建系统。因此，我们努力做到超级敏捷以确保满足客户的需求。
  其中一个最重要的策略是向客户提供的基元和工具，让客户从中选择最合适他们的集合，而不是只提供一个框架中，迫使他们不得不使用。这种做法使我们的客户取得了成功，其后的AWS服务也同样利用这些客户已经熟悉的基础服务。
  同样重要的是我们不知道客户下一个关心问题是什么，直到他们开始使用我们的服务。这就是为什么我们经常用最少的功能集提供新的服务，让我们的客户能够帮助推动产品路线图规划与新功能的开发。
 感悟： 客户的需求是下一个产品。立足基础，理解客户的需求，才能满足客户。
4. 自动化是关键  提供云计算服务不同于开发并交付软件，管理大规模系统需要不同的思维方式，以确保满足用户的高可用性，高性能和可扩展性的需求。
  这其中关键的一点是尽可能地实现自动化管理以避免容易出错的手动操作。要做到这一点，我们需要实现管理API从而管理我们的业务的关键功能。AWS可以帮助客户做到这一点。通过为应用程序的每一个分解组件提供管理API，从而应用自动化的规则来保证可靠性和预期的性能。
  如果你需要SSH到一个服务器或一个实例，那么你仍然需要更多的自动化，这是一个衡量自动化水平的好方法。
 感悟： 用人管理代码，用代码管理机器
5. APIs are forever（API是不会变的）  这是我们从亚马逊零售业务经验中吸取的教训，它的重要性甚至超过了AWS中那些以API为中心的业务。
  一旦客户开始使用我们的API构建他们的应用程序和系统，改变这些API变得不可能，因我们修改这些API,会影响客户的业务运营。我们只有一次机会定义API,所以设计API是一个非常重要的工作。
 感悟： 接口优先，实现其次，实现可以调整，而接口一旦上线就没有什么回旋的余地。
6. 监控资源应用  当构建一个服务，一定要有服务及其运营成本的数据以确定相应的计费方式，尤其是对于运行高营业额-低利润率的业务。AWS理所应当关注服务的成本，这样我们在为客户的提供服务的同时能够明确在哪些地方可以提高运营效率，以进一步削减成本，以更低的价格回馈客户。
  在创业初期，我们不知道S3服务应该采用哪种计费方式：我们曾认为应该对存储和带宽资源计费;经过一段时间，我们认识到请求的数量也应当计费。如果客户有许多微小的文件，即使他们上百万次数请求服务，消耗的存储和带宽都不会很高。我们不得不针对资源使用各个维度调整计费方式使AWS成为是一个可持续发展的业务。</description>
    </item>
    
    <item>
      <title>Facebook live一些技术细节</title>
      <link>https://myself659.github.io/2016/05/27/facebook-live%E4%B8%80%E4%BA%9B%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82/</link>
      <pubDate>Fri, 27 May 2016 11:58:06 +0200</pubDate>
      
      <guid>https://myself659.github.io/2016/05/27/facebook-live%E4%B8%80%E4%BA%9B%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82/</guid>
      <description>协议选择 最初选择HLS,后面切换为RTMP，切换为RTMP主要为了降低延迟，提供更好直播用户体验。
解决并发问题  分发架构
采用Live stream server， origin server， edge server 三层架构；如下图如示： ![分发架构图](/images/facebook live arch.png)  一句话就是：通过遍布各地的CDN节点（edge server）实现海量用户的播放请求。
 请求合并应对高并发 采用的CDN方案，应对一般数量级的播放是没有问题，但是facebook上有很多名人与网红，他们每个人都有几百万个粉丝，这就要求facebook live 直播系统能够处理超过一亿人同时播放的能力。假如一个名人的直播有100万粉丝同时观看，edge server缓存命中率为98%，那么未命中用户为2万，这2万用户回源到origin server甚至回源到Live stream server，服务器压力可想而知，这不是2W个连接而已，而是2W个视频播放，带宽，cpu都是一个很大的考验。用数字说话，以带宽为例； 假设一个HLS切片为3S，高清视频码率为1800K bps，那么一秒带宽需求为：  20K /3 *1800K b/s * 1s = 20*600 Mb = 12 Gb每秒的带宽超过10Gb，服务器网卡高配也才10Gb。
业务驱动方案，其具体解决方案如下： 在edge server 对同一个视频切片的多人cache miss请求进行合并，只发送一个到 origin server，待origin server 返回该视频切片，同时发送给该视频切片的所有请求。这样就可以大量减少回源的请求数量。origin server亦是如此。
想到下面的问题：
 最近520，林心如与霍建华在微博宣布恋爱关系，微博是怎么搞定推送信息众粉丝的呢？ 一个直播有100W在线观看，如何实现海量消息的转发？
 既然想到了，以微博为例写一下自己的方案：
 微博消息是一种pub-sub模型，简单的在问题的场景下林心如这样明星是pub，粉丝是sub，粉丝数量是7000W 微博发布了，如何发送到7000W粉丝？不可能直接将立即将消息推送到各个粉丝，不可能是一个推模型，瞬间大并发写与大量存储都会有问题，可能是拉模型吗？拉模型好处只写一条微博，新上线的用户会主动拉取订阅（关注）用户的发布的微博；僵尸用户，活不过来用户，睡眠用户是可以忽略的，怎么触发在线用户主动获取关注的人的动态保证消息的实时性呢？ 在用户线用户定时获取更新，例如60秒定时获取订阅用户是否有更新，这是一个很大的开销，假如微博有2000W同时在线，那么一秒有30多W个拉取更新的请求，每个请求都会查询订阅用户的微博，虽然可以承受的，但是代价还是很大的，同时这种定时拉取会消耗用户的流量，长期下来用户是无法忍受的，所以在线用户拉取是不可行的 对于在线用户采用推消息，7000W粉丝假设有5%用户在线，那在线粉丝为350W，如果在60S内完成，那一秒要完成60W个推送（发送60W个包，更新60W个用户订阅微博信息），如果整个系统只处理这一件事，是可以的，但是微博大V多，假如1分钟有10个类似的情况，同时也还有其他在线千万级用户（数据是乱猜的）; 系统容量是有限的，总会出现抗不住的情况的，墨菲定律告诉我们：这种情况总会发生的，只是我们不知道什么时候会发生。 抗住是目的，这时候需要作好监控，限流，扩容，降级服务保证核心功能 针对粉丝数量超过1000W以上的大V进行特殊处理  实现RTMP 选择基于nginx rtmp改造，并开发rtmp proxy，采用nginx rtmp有如下好处：</description>
    </item>
    
  </channel>
</rss>